{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPuD2rrzWiwimtvyVGhiQxO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhushan1729/Machine-Learning-Algorithm/blob/main/WGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J9mrBB42NXJh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "z_dim = 100\n",
        "batch_size = 128\n",
        "epochs = 20\n",
        "lr = 5e-5\n",
        "n_critic = 1\n",
        "clip_value = 0.01\n",
        "img_size = 28\n",
        "channels = 1\n",
        "img_shape = (channels, img_size, img_size)\n",
        "\n",
        "os.makedirs(\"wgan_dcgan\", exist_ok=True)"
      ],
      "metadata": {
        "id": "YAPBdruuNvXX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loader\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    datasets.MNIST(\n",
        "        \".\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    ),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "L2Ie5HnWPoQD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.init_size = img_size // 4  # 7x7\n",
        "        self.fc = nn.Linear(z_dim, 128 * self.init_size ** 2)\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 7 -> 14\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1),    # 14 -> 28 (fixed)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.fc(z)\n",
        "        x = x.view(z.size(0), 128, self.init_size, self.init_size)\n",
        "        return self.conv_blocks(x)\n"
      ],
      "metadata": {
        "id": "jpMbSYGlQYby"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Critic\n",
        "\"\"\"class Critic(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Conv2d(1, 64, 4, stride=2, padding=1), # 28 to 14\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Conv2d(64, 128, 4, stride=2, padding=1), # 14 to 7\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(128 * 7 * 7, 1) # Added out_features=1\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\"\"\"\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        # figure out linear input size dynamically\n",
        "        test_input = torch.zeros(1, 1, 28, 28)  # adjust if your images differ\n",
        "        out_dim = self.conv(test_input).view(1, -1).size(1)\n",
        "        self.fc = nn.Linear(out_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.flatten(x)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "JJyf3sGzRrBc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Models and Optimizers\n",
        "generator = Generator().to(device)\n",
        "critic = Critic().to(device)\n",
        "\n",
        "optimizer_G = optim.RMSprop(generator.parameters(), lr=lr)\n",
        "optimizer_D = optim.RMSprop(critic.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "R7ppaqB1SmP1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "for epoch in range(1, epochs + 1):\n",
        "  for i, (real_imgs, _) in enumerate(dataloader):\n",
        "\n",
        "    real_imgs = real_imgs.to(device)\n",
        "    b_size = real_imgs.size(0)\n",
        "\n",
        "    # Train Critic\n",
        "    for _ in range(n_critic):\n",
        "      z = torch.randn(b_size, z_dim).to(device)\n",
        "      fake_imgs = generator(z).detach()\n",
        "\n",
        "      loss_c = -torch.mean(critic(real_imgs)) + torch.mean(critic(fake_imgs))\n",
        "      critic.zero_grad()\n",
        "      loss_c.backward()\n",
        "      optimizer_D.step()\n",
        "\n",
        "      # Weight clipping for Lipschitz constraint\n",
        "      for p in critic.parameters():\n",
        "        p.data.clamp_(-clip_value, clip_value)\n",
        "\n",
        "\n",
        "    # Train Generator\n",
        "    z = torch.randn(b_size, z_dim).to(device)\n",
        "    gen_imgs = generator(z)\n",
        "    loss_g = -torch.mean(critic(gen_imgs))\n",
        "\n",
        "    optimizer_G.zero_grad()\n",
        "    loss_g.backward()\n",
        "    optimizer_G.step()\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      print(f\"Epoch [{epoch}/{epochs}] [Batch [{i}/{len(dataloader)}] \"\n",
        "      f\"[Loss Critic: {loss_c.item():.4f}] [Loss G: {loss_g.item():.4f}]\")\n",
        "\n",
        "  # Save sample every epoch\n",
        "  generator.eval()\n",
        "  with torch.no_grad():\n",
        "    z = torch.randn(64, z_dim).to(device)\n",
        "    sample = generator(z)\n",
        "    sample = sample*0.5 + 0.5 # Denormalize\n",
        "    save_image(sample, f'wgan_dcgan/epoch_{epoch}.png', nrow = 8)\n",
        "  generator.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFhRpdD5TDp6",
        "outputId": "5e7fbddd-ca90-44ae-af5f-0b91acced277"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] [Batch [0/469] [Loss Critic: -0.1524] [Loss G: 0.0155]\n",
            "Epoch [1/20] [Batch [100/469] [Loss Critic: -0.0550] [Loss G: -0.0024]\n",
            "Epoch [1/20] [Batch [200/469] [Loss Critic: -0.0376] [Loss G: -0.0274]\n",
            "Epoch [1/20] [Batch [300/469] [Loss Critic: -0.0373] [Loss G: -0.0242]\n",
            "Epoch [1/20] [Batch [400/469] [Loss Critic: -0.0359] [Loss G: -0.0270]\n",
            "Epoch [2/20] [Batch [0/469] [Loss Critic: -0.0341] [Loss G: -0.0255]\n",
            "Epoch [2/20] [Batch [100/469] [Loss Critic: -0.0348] [Loss G: -0.0263]\n",
            "Epoch [2/20] [Batch [200/469] [Loss Critic: -0.0465] [Loss G: -0.0124]\n",
            "Epoch [2/20] [Batch [300/469] [Loss Critic: -0.0662] [Loss G: 0.0035]\n",
            "Epoch [2/20] [Batch [400/469] [Loss Critic: -0.0730] [Loss G: 0.0212]\n",
            "Epoch [3/20] [Batch [0/469] [Loss Critic: -0.0902] [Loss G: 0.0441]\n",
            "Epoch [3/20] [Batch [100/469] [Loss Critic: -0.0915] [Loss G: 0.0561]\n",
            "Epoch [3/20] [Batch [200/469] [Loss Critic: -0.0966] [Loss G: 0.0343]\n",
            "Epoch [3/20] [Batch [300/469] [Loss Critic: -0.1034] [Loss G: 0.0486]\n",
            "Epoch [3/20] [Batch [400/469] [Loss Critic: -0.0967] [Loss G: 0.0175]\n",
            "Epoch [4/20] [Batch [0/469] [Loss Critic: -0.0923] [Loss G: 0.0395]\n",
            "Epoch [4/20] [Batch [100/469] [Loss Critic: -0.0979] [Loss G: 0.0200]\n",
            "Epoch [4/20] [Batch [200/469] [Loss Critic: -0.0872] [Loss G: 0.0388]\n",
            "Epoch [4/20] [Batch [300/469] [Loss Critic: -0.0844] [Loss G: 0.0118]\n",
            "Epoch [4/20] [Batch [400/469] [Loss Critic: -0.0944] [Loss G: 0.0159]\n",
            "Epoch [5/20] [Batch [0/469] [Loss Critic: -0.0862] [Loss G: 0.0305]\n",
            "Epoch [5/20] [Batch [100/469] [Loss Critic: -0.0855] [Loss G: 0.0329]\n",
            "Epoch [5/20] [Batch [200/469] [Loss Critic: -0.0801] [Loss G: 0.0480]\n",
            "Epoch [5/20] [Batch [300/469] [Loss Critic: -0.0754] [Loss G: 0.0633]\n",
            "Epoch [5/20] [Batch [400/469] [Loss Critic: -0.0769] [Loss G: 0.0326]\n",
            "Epoch [6/20] [Batch [0/469] [Loss Critic: -0.0797] [Loss G: 0.0475]\n",
            "Epoch [6/20] [Batch [100/469] [Loss Critic: -0.0771] [Loss G: 0.0624]\n",
            "Epoch [6/20] [Batch [200/469] [Loss Critic: -0.0789] [Loss G: 0.0760]\n",
            "Epoch [6/20] [Batch [300/469] [Loss Critic: -0.0841] [Loss G: 0.0416]\n",
            "Epoch [6/20] [Batch [400/469] [Loss Critic: -0.0781] [Loss G: 0.0699]\n",
            "Epoch [7/20] [Batch [0/469] [Loss Critic: -0.0794] [Loss G: 0.0683]\n",
            "Epoch [7/20] [Batch [100/469] [Loss Critic: -0.0697] [Loss G: 0.0694]\n",
            "Epoch [7/20] [Batch [200/469] [Loss Critic: -0.0773] [Loss G: 0.0640]\n",
            "Epoch [7/20] [Batch [300/469] [Loss Critic: -0.0659] [Loss G: 0.0712]\n",
            "Epoch [7/20] [Batch [400/469] [Loss Critic: -0.0695] [Loss G: 0.0578]\n",
            "Epoch [8/20] [Batch [0/469] [Loss Critic: -0.0738] [Loss G: 0.0855]\n",
            "Epoch [8/20] [Batch [100/469] [Loss Critic: -0.0711] [Loss G: 0.0869]\n",
            "Epoch [8/20] [Batch [200/469] [Loss Critic: -0.0664] [Loss G: 0.0578]\n",
            "Epoch [8/20] [Batch [300/469] [Loss Critic: -0.0725] [Loss G: 0.0634]\n",
            "Epoch [8/20] [Batch [400/469] [Loss Critic: -0.0649] [Loss G: 0.0744]\n",
            "Epoch [9/20] [Batch [0/469] [Loss Critic: -0.0696] [Loss G: 0.0695]\n",
            "Epoch [9/20] [Batch [100/469] [Loss Critic: -0.0640] [Loss G: 0.0568]\n",
            "Epoch [9/20] [Batch [200/469] [Loss Critic: -0.0672] [Loss G: 0.0677]\n",
            "Epoch [9/20] [Batch [300/469] [Loss Critic: -0.0667] [Loss G: 0.0469]\n",
            "Epoch [9/20] [Batch [400/469] [Loss Critic: -0.0611] [Loss G: 0.0453]\n",
            "Epoch [10/20] [Batch [0/469] [Loss Critic: -0.0592] [Loss G: 0.0363]\n",
            "Epoch [10/20] [Batch [100/469] [Loss Critic: -0.0665] [Loss G: 0.0504]\n",
            "Epoch [10/20] [Batch [200/469] [Loss Critic: -0.0594] [Loss G: 0.0614]\n",
            "Epoch [10/20] [Batch [300/469] [Loss Critic: -0.0572] [Loss G: 0.0503]\n",
            "Epoch [10/20] [Batch [400/469] [Loss Critic: -0.0599] [Loss G: 0.0507]\n",
            "Epoch [11/20] [Batch [0/469] [Loss Critic: -0.0587] [Loss G: 0.0258]\n",
            "Epoch [11/20] [Batch [100/469] [Loss Critic: -0.0570] [Loss G: 0.0481]\n",
            "Epoch [11/20] [Batch [200/469] [Loss Critic: -0.0567] [Loss G: 0.0360]\n",
            "Epoch [11/20] [Batch [300/469] [Loss Critic: -0.0564] [Loss G: 0.0244]\n",
            "Epoch [11/20] [Batch [400/469] [Loss Critic: -0.0508] [Loss G: 0.0664]\n",
            "Epoch [12/20] [Batch [0/469] [Loss Critic: -0.0485] [Loss G: 0.0511]\n",
            "Epoch [12/20] [Batch [100/469] [Loss Critic: -0.0420] [Loss G: 0.0317]\n",
            "Epoch [12/20] [Batch [200/469] [Loss Critic: -0.0508] [Loss G: 0.0457]\n",
            "Epoch [12/20] [Batch [300/469] [Loss Critic: -0.0483] [Loss G: 0.0288]\n",
            "Epoch [12/20] [Batch [400/469] [Loss Critic: -0.0444] [Loss G: 0.0342]\n",
            "Epoch [13/20] [Batch [0/469] [Loss Critic: -0.0467] [Loss G: 0.0556]\n",
            "Epoch [13/20] [Batch [100/469] [Loss Critic: -0.0412] [Loss G: 0.0576]\n",
            "Epoch [13/20] [Batch [200/469] [Loss Critic: -0.0458] [Loss G: 0.0439]\n",
            "Epoch [13/20] [Batch [300/469] [Loss Critic: -0.0380] [Loss G: 0.0309]\n",
            "Epoch [13/20] [Batch [400/469] [Loss Critic: -0.0380] [Loss G: 0.0354]\n",
            "Epoch [14/20] [Batch [0/469] [Loss Critic: -0.0423] [Loss G: 0.0418]\n",
            "Epoch [14/20] [Batch [100/469] [Loss Critic: -0.0427] [Loss G: 0.0589]\n",
            "Epoch [14/20] [Batch [200/469] [Loss Critic: -0.0395] [Loss G: 0.0128]\n",
            "Epoch [14/20] [Batch [300/469] [Loss Critic: -0.0346] [Loss G: 0.0132]\n",
            "Epoch [14/20] [Batch [400/469] [Loss Critic: -0.0438] [Loss G: 0.0366]\n",
            "Epoch [15/20] [Batch [0/469] [Loss Critic: -0.0410] [Loss G: 0.0521]\n",
            "Epoch [15/20] [Batch [100/469] [Loss Critic: -0.0440] [Loss G: 0.0351]\n",
            "Epoch [15/20] [Batch [200/469] [Loss Critic: -0.0362] [Loss G: 0.0215]\n",
            "Epoch [15/20] [Batch [300/469] [Loss Critic: -0.0358] [Loss G: 0.0112]\n",
            "Epoch [15/20] [Batch [400/469] [Loss Critic: -0.0384] [Loss G: 0.0048]\n",
            "Epoch [16/20] [Batch [0/469] [Loss Critic: -0.0278] [Loss G: 0.0094]\n",
            "Epoch [16/20] [Batch [100/469] [Loss Critic: -0.0333] [Loss G: 0.0274]\n",
            "Epoch [16/20] [Batch [200/469] [Loss Critic: -0.0383] [Loss G: 0.0227]\n",
            "Epoch [16/20] [Batch [300/469] [Loss Critic: -0.0335] [Loss G: 0.0404]\n",
            "Epoch [16/20] [Batch [400/469] [Loss Critic: -0.0305] [Loss G: 0.0070]\n",
            "Epoch [17/20] [Batch [0/469] [Loss Critic: -0.0310] [Loss G: 0.0210]\n",
            "Epoch [17/20] [Batch [100/469] [Loss Critic: -0.0321] [Loss G: 0.0044]\n",
            "Epoch [17/20] [Batch [200/469] [Loss Critic: -0.0349] [Loss G: 0.0056]\n",
            "Epoch [17/20] [Batch [300/469] [Loss Critic: -0.0351] [Loss G: 0.0519]\n",
            "Epoch [17/20] [Batch [400/469] [Loss Critic: -0.0385] [Loss G: 0.0367]\n",
            "Epoch [18/20] [Batch [0/469] [Loss Critic: -0.0360] [Loss G: 0.0345]\n",
            "Epoch [18/20] [Batch [100/469] [Loss Critic: -0.0219] [Loss G: -0.0039]\n",
            "Epoch [18/20] [Batch [200/469] [Loss Critic: -0.0334] [Loss G: 0.0140]\n",
            "Epoch [18/20] [Batch [300/469] [Loss Critic: -0.0304] [Loss G: 0.0375]\n",
            "Epoch [18/20] [Batch [400/469] [Loss Critic: -0.0294] [Loss G: 0.0164]\n",
            "Epoch [19/20] [Batch [0/469] [Loss Critic: -0.0339] [Loss G: 0.0007]\n",
            "Epoch [19/20] [Batch [100/469] [Loss Critic: -0.0312] [Loss G: 0.0438]\n",
            "Epoch [19/20] [Batch [200/469] [Loss Critic: -0.0331] [Loss G: -0.0060]\n",
            "Epoch [19/20] [Batch [300/469] [Loss Critic: -0.0297] [Loss G: 0.0035]\n",
            "Epoch [19/20] [Batch [400/469] [Loss Critic: -0.0328] [Loss G: 0.0091]\n",
            "Epoch [20/20] [Batch [0/469] [Loss Critic: -0.0324] [Loss G: 0.0035]\n",
            "Epoch [20/20] [Batch [100/469] [Loss Critic: -0.0305] [Loss G: 0.0311]\n",
            "Epoch [20/20] [Batch [200/469] [Loss Critic: -0.0311] [Loss G: -0.0010]\n",
            "Epoch [20/20] [Batch [300/469] [Loss Critic: -0.0321] [Loss G: 0.0273]\n",
            "Epoch [20/20] [Batch [400/469] [Loss Critic: -0.0320] [Loss G: -0.0031]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect one batch\n",
        "real_imgs, _ = next(iter(dataloader))\n",
        "print(\"Dataset batch shape:\", real_imgs.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtqHKHK1ZHbZ",
        "outputId": "628e64fc-e98c-45a2-bc1f-5c6db3ec6926"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset batch shape: torch.Size([128, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.randn(1, z_dim).to(device)\n",
        "gen_img = generator(z)\n",
        "print(\"Generator output shape:\", gen_img.shape)\n",
        "\n",
        "crit_out = critic.conv(gen_img)\n",
        "print(\"Critic conv output shape:\", crit_out.shape)\n",
        "print(\"Flattened size:\", crit_out.view(1, -1).size(1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiuDX6YSZH9m",
        "outputId": "15624d73-b25e-4317-e796-270d2de0a0dc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator output shape: torch.Size([1, 1, 28, 28])\n",
            "Critic conv output shape: torch.Size([1, 128, 7, 7])\n",
            "Flattened size: 6272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x5OnvG-hZlwA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}